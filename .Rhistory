diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
#d
install.packages("glmnet")
library(glmnet)
# Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
grid
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
ridge_cv_fit
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# Extract corresponding mean MSE
ridge_cv_fit$cvm[i]
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit_1 = glmnet(X,y, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
install.packages("mlbench")
# Load mlbench package
library(mlbench)
# Load the data
data(BreastCancer)
# Check size
dim(BreastCancer)
# Print first few rows
head(BreastCancer)
mode(BreastCancer)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
na.omit(is.na(BreastCancer[24,])=TRUE)
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
na.omit(is.na(BreastCancer[24,])==TRUE)
dim(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
#d
install.packages("glmnet")
library(glmnet)
# Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
grid
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
ridge_cv_fit
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# Extract corresponding mean MSE
ridge_cv_fit$cvm[i]
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit_1 = glmnet(X,y, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
diabete_data_test=as.data.frame(data_test[,1:10])
y_data_test=data_test[,11]
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
## Summarise model fit:
summary(dis_fit)
attr(X,"scaled:scale")
par(mfrow=c(1,2))
plot(dis_fit,which=1:2)
## Compute fitted values for test data:
yhat_test = predict(dis_fit, test)
head(yhat_test)
#Compute test error
test_error = mean((test$y_data_test - yhat_test)^2)
test_error
#c
fit= lm(y_data_train~sex+bmi+map+tc+ldl+ltg,train)
summary(fit)
## Compute fitted values for test data:
yhat_test_1 = predict(fit, test)
head(yhat_test_1)
#Compute test error
test_error_1= mean((test$y_data_test - yhat_test_1)^2)
test_error_1
#d
install.packages("glmnet")
install.packages("glmnet")
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(audit2),round(nrow(audit2)*7/10))
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
data_train=audit2[sub,]#取2/3的数据做训练集
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
data_train=BreastCancer[sub,]#取2/3的数据做训练集
data_test=BreastCancer[-sub,]#取1/3的数据做测试集
dim(data_train)#训练集行数和列数13542 23
dim(data_test) #测试集的行数和列数6771 23
install.packages("mlbench")
# Load mlbench package
library(mlbench)
# Load the data
data(BreastCancer)
# Check size
dim(BreastCancer)
# Print first few rows
head(BreastCancer)
mode(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
#取7/10的数据做训练集
data_train=BreastCancer[sub,]
#取3/10的数据做测试集
data_test=BreastCancer[-sub,]
#训练集行数和列数478 11
dim(data_train)
#测试集的行数和列数205 11
dim(data_test)
#看该列分布的
table(data_train$是否转化)
mode(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
#取7/10的数据做训练集
data_train=BreastCancer[sub,]
#取3/10的数据做测试集
data_test=BreastCancer[-sub,]
#训练集行数和列数478 11
dim(data_train)
#测试集的行数和列数205 11
dim(data_test)
## Extract response variable:We see that the response variable (dis) is stored in column 11 and that the explanatory variables appear in columns 1–10.
y = diabetes[,11]
# Extract explanatory variables:
X_raw = diabetes[,2:10]
class(X_raw)
## Convert to matrix:
X_raw = as.matrix(X_raw)
class(X_raw)
# Standardise explanatory variables
X = scale(X_raw)
## Print the first 5 rows: head(X, 5)
head(X,5)
## Create single data frame containing response and explanatory variables:
diabetes_data = data.frame(y, X)
# Fit linear model:
diabete_data_train=as.data.frame(data_train[,1:10])
y_data_train=data_train[,11]
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=data_test[,11]
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
y_data_train=data.frame(data_train[,11])
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=data.frame(data_test[,11])
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
## Summarise model fit:
summary(dis_fit)
attr(X,"scaled:scale")
## Create single data frame containing response and explanatory variables:
diabetes_data = data.frame(y, X)
# Fit linear model:
diabete_data_train=as.data.frame(data_train[,2:10])
dim(diabete_data_train)
y_data_train=list(data_train[,11])
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=list(data_test[,11])
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
getwd()
install.packages("backports")
library(tidyverse)
install.packages("mass")
install.packages("MASS")
library(MASS)
library(tidyverse)
detach("package:base", unload = TRUE)
remove.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
remove.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
remove.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
remove.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
install.packages("backports")
install.packages("backports")
library(tidyverse)
setwd("/Users/wangyahan/Desktop/Data Management and Explor/DM")
library('ProjectTemplate')
load.project()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=normalizePath('..'))
tinytex::install_tinytex()
library(ProjectTemplate)
load.project()
ggplot(age_range,aes(x=age_range, fill=age_range))+geom_bar()
ggplot(highest_education_level,aes(x=highest_education_level, fill=highest_education_level))+geom_bar()
ggplot(employment_status,aes(x=employment_status, fill=employment_status))+geom_bar()
ggplot(employment_area,aes(x=employment_area, fill=employment_area))+geom_bar()
country_1 = data.frame(country_1)
ggplot(country_1,aes(x=country_1[,1],y=country_1[,2],fill=country_1[,1])) +
geom_bar(stat = "identity")
detected_country_1 = data.frame(detected_country_1)
ggplot(detected_country_1,aes(x=detected_country_1[,1],y=detected_country_1[,2],fill=detected_country_1[,1])) +
geom_bar(stat = "identity")
ggplot(t, aes(x=quiz_question,y=answers_numbers,fill=answers_numbers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture=ggplot(t_1, aes(x=question,y=No.people,fill=No.people))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture+scale_fill_gradient(low='white',high='red')+ #Gradient colour fill, from white to red
theme_bw()
pictire_1=ggplot(rep, aes(x=question_1,y=repeated_answers,fill=repeated_answers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
pictire_1 +scale_fill_gradient(low='pink',high='green')+
theme_bw()
ggplot(t_2, aes(x = step, y = numbers)) + geom_boxplot()
ggplot (t_2, aes (x = step, y = numbers, size = numbers, colour =numbers)) +
# Scatter function: alpha sets the transparency of the scatter.
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable.
scale_size_area () +scale_color_gradient2(low = "yellow", mid = "green", high = "brown")
ggplot(t_3, aes(x = step, y = numbers_null)) +  geom_boxplot()
ggplot (t_3, aes (x = step, y = numbers_null, size = numbers_null, colour =numbers_null)) +
#  Scatter plot function: alpha sets the scatter transparency
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable
scale_size_area () +scale_color_gradient2(low = "pink", mid = "red", high = "blue")
ggplot(t_4, aes(x = step, y = Percentage)) + geom_boxplot()
ggplot (t_4, aes (x = step, y = Percentage, size = Percentage, colour =Percentage)) +
# Scatter plot function: alpha sets the scatter transparency
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable
scale_size_area () +scale_color_gradient2(low = "blue", mid = "orange", high = "purple")
data1 = data.frame(gender=as.numeric(United_1$gender),age_range=as.numeric(United_1$age_range),
highest_education_level=as.numeric(United_1$highest_education_level),
Number_of_courses=as.numeric(United_1$Number_of_courses))
view(data1)
fit.full=lm(Number_of_courses~gender+age_range+highest_education_level,data=data1)
data1
summary(fit.full)
plot(fit.full)
plot(data1$Number_of_courses,predict(fit.full,data1))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=normalizePath('..'))
library(ProjectTemplate)
load.project()
install.packages('rticles')
library(ProjectTemplate)
load.project()
install.packages('rticles')
ggplot(age_range,aes(x=age_range, fill=age_range))+geom_bar()
ggplot(highest_education_level,aes(x=highest_education_level, fill=highest_education_level))+geom_bar()
ggplot(employment_status,aes(x=employment_status, fill=employment_status))+geom_bar()
ggplot(employment_area,aes(x=employment_area, fill=employment_area))+geom_bar()
country_1 = data.frame(country_1)
ggplot(country_1,aes(x=country_1[,1],y=country_1[,2],fill=country_1[,1])) +
geom_bar(stat = "identity")
detected_country_1 = data.frame(detected_country_1)
ggplot(detected_country_1,aes(x=detected_country_1[,1],y=detected_country_1[,2],fill=detected_country_1[,1])) +
geom_bar(stat = "identity")
ggplot(t, aes(x=quiz_question,y=answers_numbers,fill=answers_numbers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture=ggplot(t_1, aes(x=question,y=No.people,fill=No.people))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture+scale_fill_gradient(low='white',high='red')+ #Gradient colour fill, from white to red
theme_bw()
pictire_1=ggplot(rep, aes(x=question_1,y=repeated_answers,fill=repeated_answers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
pictire_1 +scale_fill_gradient(low='pink',high='green')+
theme_bw()
ggplot(t_2, aes(x = step, y = numbers)) + geom_boxplot()
ggplot (t_2, aes (x = step, y = numbers, size = numbers, colour =numbers)) +
# Scatter function: alpha sets the transparency of the scatter.
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable.
scale_size_area () +scale_color_gradient2(low = "yellow", mid = "green", high = "brown")
ggplot(t_3, aes(x = step, y = numbers_null)) +  geom_boxplot()
ggplot (t_3, aes (x = step, y = numbers_null, size = numbers_null, colour =numbers_null)) +
#  Scatter plot function: alpha sets the scatter transparency
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable
scale_size_area () +scale_color_gradient2(low = "pink", mid = "red", high = "blue")
ggplot(t_4, aes(x = step, y = Percentage)) + geom_boxplot()
ggplot (t_4, aes (x = step, y = Percentage, size = Percentage, colour =Percentage)) +
# Scatter plot function: alpha sets the scatter transparency
geom_point (position = "jitter") +
# Make the area of the scatter positively proportional to the value of the variable
scale_size_area () +scale_color_gradient2(low = "blue", mid = "orange", high = "purple")
data1 = data.frame(gender=as.numeric(United_1$gender),age_range=as.numeric(United_1$age_range),
highest_education_level=as.numeric(United_1$highest_education_level),
Number_of_courses=as.numeric(United_1$Number_of_courses))
view(data1)
fit.full=lm(Number_of_courses~gender+age_range+highest_education_level,data=data1)
data1
summary(fit.full)
plot(fit.full)
plot(data1$Number_of_courses,predict(fit.full,data1))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=normalizePath('..'))
install.packages('rticles')
install.packages('rticles')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir=normalizePath('..'))
library(ProjectTemplate)
load.project()
ggplot(age_range,aes(x=age_range, fill=age_range))+geom_bar()
ggplot(highest_education_level,aes(x=highest_education_level, fill=highest_education_level))+geom_bar()
ggplot(employment_status,aes(x=employment_status, fill=employment_status))+geom_bar()
ggplot(employment_area,aes(x=employment_area, fill=employment_area))+geom_bar()
country_1 = data.frame(country_1)
ggplot(country_1,aes(x=country_1[,1],y=country_1[,2],fill=country_1[,1])) +
geom_bar(stat = "identity")
detected_country_1 = data.frame(detected_country_1)
ggplot(detected_country_1,aes(x=detected_country_1[,1],y=detected_country_1[,2],fill=detected_country_1[,1])) +
geom_bar(stat = "identity")
ggplot(t, aes(x=quiz_question,y=answers_numbers,fill=answers_numbers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture=ggplot(t_1, aes(x=question,y=No.people,fill=No.people))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
picture+scale_fill_gradient(low='white',high='red')+ #Gradient colour fill, from white to red
theme_bw()
pictire_1=ggplot(rep, aes(x=question_1,y=repeated_answers,fill=repeated_answers))+geom_bar(stat="identity")+coord_polar(theta = "x", direction=1)
pictire_1 +scale_fill_gradient(low='pink',high='green')+
theme_bw()
data1 = data.frame(gender=as.numeric(United_1$gender),age_range=as.numeric(United_1$age_range),
highest_education_level=as.numeric(United_1$highest_education_level),
Number_of_courses=as.numeric(United_1$Number_of_courses))
data1
data1=na.omit(data1)
fit.full=lm(Number_of_courses~gender+age_range+highest_education_level,data=data1)
data1
summary(fit.full)
plot(fit.full)
