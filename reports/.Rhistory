# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
ridge_cv_fit
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
# Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
grid
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
ridge_cv_fit
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# Extract corresponding mean MSE
ridge_cv_fit$cvm[i]
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit_1 = glmnet(X,y, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
#d
install.packages("glmnet")
library(glmnet)
# Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
grid
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
ridge_cv_fit
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# Extract corresponding mean MSE
ridge_cv_fit$cvm[i]
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit_1 = glmnet(X,y, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
install.packages("mlbench")
# Load mlbench package
library(mlbench)
# Load the data
data(BreastCancer)
# Check size
dim(BreastCancer)
# Print first few rows
head(BreastCancer)
mode(BreastCancer)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
na.omit(is.na(BreastCancer[24,])=TRUE)
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
na.omit(is.na(BreastCancer[24,])==TRUE)
dim(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
#d
install.packages("glmnet")
library(glmnet)
# Choose grid of values for the tuning parameter
grid = 10^seq(5, -3, length=100)
grid
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit = glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit = cv.glmnet(diabete_data_train, y_data_train, alpha=0, standardize=FALSE, lambda=grid)
ridge_cv_fit
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(ridge_cv_fit$lambda == ridge_cv_fit$lambda.min))
# Extract corresponding mean MSE
ridge_cv_fit$cvm[i]
#chosen value of λ from the ridge fit object that we fitted to the full data set using the coef function. We need to use the s argument to set the tuning parameter equal to our chosen value:
coef(ridge_fit, s=lambda_min)
#Finally we can use the predict function to perform prediction at our new set of explanatory variables x:
predict(ridge_fit, newx=as.matrix(x), s=lambda_min)
## Fit a ridge regression model for each value of the tuning parameter
ridge_fit_1 = glmnet(X,y, alpha=0, standardize=FALSE, lambda=grid)
#We can extract the ridge regression coefficients βˆ1r, . . . , βˆpr using the coef function:
beta_hat = coef(ridge_fit)
length(grid)
dim(beta_hat)
plot(ridge_fit_1, xvar="lambda", col=1:10, label=TRUE)
#we can use the cv.glmnet function as follows:
diabete_data_train_1=data_train[,1:10]
diabete_data_train=as.matrix(data_train[,1:10])
ridge_cv_fit_1 = cv.glmnet(X, y, alpha=0, standardize=FALSE, lambda=grid)
#We can display how the cross-validated error varies with λ as follows
plot(ridge_cv_fit_1)
#We can extract the value of λ corresponding to the minimum using
(lambda_min = ridge_cv_fit_1$lambda.min)
## Fit a LASSO regression for each value of the tuning parameter
lasso_fit = glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid)
#use the coef function which gives a matrix whose ith column corresponds to the ith value of the tuning parameter:
beta_hat = coef(lasso_fit)
## Lots of shrinkage
grid[1]
beta_hat[,1]
# Some shrinkage
grid[75]
beta_hat[,75]
#Very little shrinkage
grid[100]
beta_hat[,100]
plot(lasso_fit, xvar="lambda", col=1:10, label=TRUE)
# Fit model, applying 10-fold cross validation:
lasso_cv_fit = cv.glmnet(X, y, alpha=1, standardize=FALSE, lambda=grid) ## Plot the cross-validation scores:
plot(lasso_cv_fit)
# Extract the optimal value of the tuning parameter
(lambda_min = lasso_cv_fit$lambda.min)
# Which tuning parameter was the minimum?
(i = which(lasso_cv_fit$lambda == lasso_cv_fit$lambda.min))
#Extract corresponding mean MSE
lasso_cv_fit$cvm[i]
#The regression coefficients obtained by performing the LASSO with the chosen value of λ are:
coef(lasso_fit, s=lambda_min)
diabete_data_test=as.data.frame(data_test[,1:10])
y_data_test=data_test[,11]
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
## Summarise model fit:
summary(dis_fit)
attr(X,"scaled:scale")
par(mfrow=c(1,2))
plot(dis_fit,which=1:2)
## Compute fitted values for test data:
yhat_test = predict(dis_fit, test)
head(yhat_test)
#Compute test error
test_error = mean((test$y_data_test - yhat_test)^2)
test_error
#c
fit= lm(y_data_train~sex+bmi+map+tc+ldl+ltg,train)
summary(fit)
## Compute fitted values for test data:
yhat_test_1 = predict(fit, test)
head(yhat_test_1)
#Compute test error
test_error_1= mean((test$y_data_test - yhat_test_1)^2)
test_error_1
#d
install.packages("glmnet")
install.packages("glmnet")
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(audit2),round(nrow(audit2)*7/10))
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
data_train=audit2[sub,]#取2/3的数据做训练集
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
data_train=BreastCancer[sub,]#取2/3的数据做训练集
data_test=BreastCancer[-sub,]#取1/3的数据做测试集
dim(data_train)#训练集行数和列数13542 23
dim(data_test) #测试集的行数和列数6771 23
install.packages("mlbench")
# Load mlbench package
library(mlbench)
# Load the data
data(BreastCancer)
# Check size
dim(BreastCancer)
# Print first few rows
head(BreastCancer)
mode(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
#取7/10的数据做训练集
data_train=BreastCancer[sub,]
#取3/10的数据做测试集
data_test=BreastCancer[-sub,]
#训练集行数和列数478 11
dim(data_train)
#测试集的行数和列数205 11
dim(data_test)
#看该列分布的
table(data_train$是否转化)
mode(BreastCancer)
#convert the factors to quantitative variables
X_raw= apply(X_raw,2,as.numeric)
# Print 24th row of Breast Cancer data and note there is a NA in the
## Bare.nuclei column:
BreastCancer[24,]
# Test whether each element on the 24th row is a NA:
(is.na(BreastCancer[24,]))
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
set.seed(1)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
length(BreastCancer)
#取7/10的数据做训练集
data_train=BreastCancer[sub,]
#取3/10的数据做测试集
data_test=BreastCancer[-sub,]
#训练集行数和列数478 11
dim(data_train)
#测试集的行数和列数205 11
dim(data_test)
## Extract response variable:We see that the response variable (dis) is stored in column 11 and that the explanatory variables appear in columns 1–10.
y = diabetes[,11]
# Extract explanatory variables:
X_raw = diabetes[,2:10]
class(X_raw)
## Convert to matrix:
X_raw = as.matrix(X_raw)
class(X_raw)
# Standardise explanatory variables
X = scale(X_raw)
## Print the first 5 rows: head(X, 5)
head(X,5)
## Create single data frame containing response and explanatory variables:
diabetes_data = data.frame(y, X)
# Fit linear model:
diabete_data_train=as.data.frame(data_train[,1:10])
y_data_train=data_train[,11]
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=data_test[,11]
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
y_data_train=data.frame(data_train[,11])
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=data.frame(data_test[,11])
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
## Summarise model fit:
summary(dis_fit)
attr(X,"scaled:scale")
## Create single data frame containing response and explanatory variables:
diabetes_data = data.frame(y, X)
# Fit linear model:
diabete_data_train=as.data.frame(data_train[,2:10])
dim(diabete_data_train)
y_data_train=list(data_train[,11])
dim(y_data_train)
diabete_data_test=as.data.frame(data_test[,2:10])
y_data_test=list(data_test[,11])
train=data.frame(y_data_train,diabete_data_train)
dim(train)
test=data.frame(y_data_test,diabete_data_test)
dis_fit =lm(y_data_train~.,train)
library(ProjectTemplate)
install.packages("ProjectTemplate")
library(ProjectTemplate)
setwd("~/Desktop/Data Management and Explor")
create.project("DM")
setwd("DM/reports")
knitr::opts_chunk$set(echo = TRUE)
library("ProjectTemplate")
load.project()
library("ProjectTemplate")
migrate.project()
library("ProjectTemplate")
load.project()
library("ProjectTemplate")
load.project()
library("ProjectTemplate")
load.project()
setwd("DM/reports")
library("ProjectTemplate")
load.project()
install.packages("ProjectTemplate")
install.packages("ProjectTemplate")
library(ProjectTemplate)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
library("ProjectTemplate")
load.project()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
library(ProjectTemplate)
load.project()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
install.packages("ProjectTemplate")
library(ProjectTemplate)
load.project()
install.packages("ProjectTemplate")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
install.packages("ProjectTemplate")
library(ProjectTemplate)
load.project()
library(ProjectTemplate)
load.project()
install.packages("ProjectTemplate")
install.packages("ProjectTemplate")
library(ProjectTemplate)
setwd("~/Desktop/Data Management and Explor")
create.project("DM")
setwd("DM/reports")
##### Training and testing
## logistic regression
logit_fit =glm(Class~.,data = data_train, family = "binomial")
# Load mlbench package
library(mlbench)
library(nclSLR)
library(MASS)
# Load the data
data(BreastCancer)
head(BreastCancer)
dim(BreastCancer)
# Remove NA
BreastCancer=na.omit(BreastCancer)
dim(BreastCancer)
#convert the factors to quantitative variables
MyBreastCancer.x = apply(BreastCancer[,2:10],2,as.numeric)
MyBreastCancer.y = as.numeric(BreastCancer[,11])-1
MyBreastCancer = data.frame(MyBreastCancer.x, Class=MyBreastCancer.y)
head(MyBreastCancer)
#Set train and test set////////////////////////////////////////////
set.seed(498)
sub= sample(1:nrow(BreastCancer),round(nrow(BreastCancer)*7/10))
#7/10 data for training
data_train=MyBreastCancer[sub,]
MyBreastCancer_data_train=as.data.frame(data_train[,1:9])
